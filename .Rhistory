library(bonsai)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>% # might need to install
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Grid of values to tune over
grid_of_tuning_params <- grid_regular(trees(range = c(50, 300)), levels = 4)
## Split data for CV
folds <- vfold_cv(bike_data, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=grid_of_tuning_params,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
# Finalize the Workflow & fit it
final_wf <-
preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_data)
# Predict on test set
bike_predictions <- predict(final_wf, new_data = test_data) %>%
mutate(.pred = exp(.pred) - 1,
.pred = pmax(0, .pred))
# Kaggle submission
kaggle_submission <- test_data %>%
select(datetime) %>%
bind_cols(bike_predictions) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
vroom_write(kaggle_submission, "./BARTPreds.csv", delim = ",")
library(agua)
library(bonsai)
library(lightgbm)
library(lightgbm)
library(ranger)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(glmnet)
bike_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\train.csv") %>% select(-casual, -registered) %>%
mutate(count_log = log1p(count)) %>%
select(-count)
test_data <- vroom("C:\\Users\\mitch\\OneDrive\\Documents\\GitHub\\KaggleBikeShare\\test.csv")
# Recipe
my_recipe <- recipe(count_log ~ ., data = bike_data) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather),
weekend = ifelse(wday(datetime) %in% c(1,7), 1, 0),
rush_hour = ifelse(hour(datetime) %in% c(7:9,16:19), 1, 0)) %>%
step_date(datetime, features = c("month", "year", "dow")) %>%
step_time(datetime, features = "hour") %>%
step_rm(datetime) %>%
step_interact(terms = ~ temp:humidity + temp:windspeed + season:humidity) %>%
step_poly(temp, humidity, degree = 2) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Initialize an h2o session
h2o::h2o.init()
qt(p = .95, df = 8, lower.tail = TRUE)
qt(p = .05, df = 8, lower.tail = TRUE)
MOE <- qt(0.975, df = 26) * (126.3 / (sqrt(27)))
lower bound <- 364.3 - MOE
lower_bound <- 364.3 - MOE
upper_bound <- 364.3 + MOE
qt(0.975, df = 26)
setwd("~/GitHub/GhoulsGoblinsGhosts")
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
library(glmnet)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
library(embed)
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
# Feature Engineering
train_data <- train_data %>%
mutate(
type = as.factor(type),
color = as.factor(color),
across(where(is.numeric) & !all_of("type"), as.factor)
)
test_data <- test_data %>%
mutate(type = as.factor(type),
color = as.factor(color),
across(where(is.numeric), as.factor))
test_data <- test_data %>%
mutate(color = as.factor(color),
across(where(is.numeric), as.factor))
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
# Feature Engineering
train_data <- train_data %>%
mutate(
type = as.factor(type),
color = as.factor(color))
test_data <- test_data %>%
mutate(color = as.factor(color))
# Feature Engineering
train_data <- train_data %>%
mutate(
type = as.factor(type),
color = as.factor(color))
test_data <- test_data %>%
mutate(color = as.factor(color))
# Recipe
my_recipe <- recipe(type ~ ., data = train_data) %>%
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_normalize(all_predictors())
rf_mod <- rand_forest( mtry = tune(), min_n = tune(), trees = 500 ) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(mtry(),
min_n(),
levels = 3 )
## Split data for CV
folds <- vfold_cv(train_data, v = 3)
metrics_multiclass <- metric_set(accuracy, mn_log_loss, roc_auc)
CV_results <- rf_wf %>%
tune_grid(
resamples = folds,
grid = tuning_grid,
metrics = metrics_multiclass,
control = control_grid(save_pred = TRUE)
)
## Grid of values to tune over
tuning_grid <- grid_regular(mtry(),
min_n(),
levels = 3 )
rf_mod <- rand_forest( mtry = tune(), min_n = tune(), trees = 500 ) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(mtry(),
min_n(),
levels = 3 )
rlang::last_trace()
# Recipe
my_recipe <- recipe(type ~ ., data = train_data) %>%
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_normalize(all_predictors())
rf_mod <- rand_forest( mtry = tune(), min_n = tune(), trees = 500 ) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(mtry(),
min_n(),
levels = 3 )
library(dials)
# Create parameter object from your model
rf_params <- parameters(rf_mod)
rf_params <- rf_wf %>%
parameters() %>%
finalize(my_recipe %>% prep(training = train_data))
rf_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_mod)
rf_params <- rf_wf %>%
parameters() %>%
finalize(my_recipe %>% prep(training = train_data))
library(tune)
rf_params <- parameters(
mtry(),
min_n()
)
# Finalize mtry based on processed predictors
rf_params <- finalize(rf_params, my_recipe %>% prep(training = train_data))
tuning_grid <- grid_regular(
mtry(range = c(1, 10)),
min_n(range = c(2, 10)),
levels = 3
)
## Split data for CV
folds <- vfold_cv(train_data, v = 3)
metrics_multiclass <- metric_set(accuracy, mn_log_loss, roc_auc)
CV_results <- rf_wf %>%
tune_grid(
resamples = folds,
grid = tuning_grid,
metrics = metrics_multiclass,
control = control_grid(save_pred = TRUE)
)
## Find Best Tuning Parameters
bestTune <- CV_results %>% select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <- rf_wf %>%
finalize_workflow(bestTune)%>%
fit(data=train_data)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id))
# (Optional) Predict probabilities
final_prob_predictions <- final_wf %>%
predict(new_data = test_data, type = "prob") %>%
bind_cols(test_data %>% select(id))
vroom_write(x = final_predictions, file = "./ggg_rf_model_preds.csv", delim = ",")
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id)) %>%
rename(type = .pred_1) %>%
select(id, type)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id)) %>%
final_predictions <- final_predictions %>%
rename(type = .pred_1) %>%
select(id, type)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id)) %>%
final_predictions <- final_predictions %>%
rename(type = .pred_class) %>%
select(id, type)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id)) %>%
rename(type = .pred_class) %>%
select(id, type)
vroom_write(x = final_predictions, file = "./ggg_rf_model_preds.csv", delim = ",")
View(train_data)
DataExplorer::plot_correlation(train_data)
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
# Feature Engineering
train_data <- train_data %>%
mutate(
type = as.factor(type),
color = as.factor(color))
test_data <- test_data %>%
mutate(color = as.factor(color))
# Recipe
my_recipe <- recipe(type ~ ., data = train_data) %>%
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_interact(terms = ~ type_Ghost:hair_length) %>%
step_normalize(all_predictors())
DataExplorer::plot_correlation(train_data)
rf_mod <- rand_forest( mtry = tune(), min_n = tune(), trees = 500 ) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_mod)
tuning_grid <- grid_regular(
mtry(range = c(1, 10)),
min_n(range = c(2, 10)),
levels = 4
)
## Split data for CV
folds <- vfold_cv(train_data, v = 4)
metrics_multiclass <- metric_set(accuracy, mn_log_loss, roc_auc)
CV_results <- rf_wf %>%
tune_grid(
resamples = folds,
grid = tuning_grid,
metrics = metrics_multiclass,
control = control_grid(save_pred = TRUE)
)
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
# Feature Engineering
train_data <- train_data %>%
mutate(
type = as.factor(type),
color = as.factor(color))
test_data <- test_data %>%
mutate(color = as.factor(color))
# Recipe
my_recipe <- recipe(type ~ ., data = train_data) %>%
update_role(id, new_role = "id variable") %>%
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_normalize(all_predictors())
DataExplorer::plot_correlation(train_data)
rf_mod <- rand_forest( mtry = tune(), min_n = tune(), trees = 500 ) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rf_mod)
tuning_grid <- grid_regular(
mtry(range = c(1, 10)),
min_n(range = c(2, 10)),
levels = 4
)
## Split data for CV
folds <- vfold_cv(train_data, v = 4)
metrics_multiclass <- metric_set(accuracy, mn_log_loss, roc_auc)
CV_results <- rf_wf %>%
tune_grid(
resamples = folds,
grid = tuning_grid,
metrics = metrics_multiclass,
control = control_grid(save_pred = TRUE)
)
## Find Best Tuning Parameters
bestTune <- CV_results %>% select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <- rf_wf %>%
finalize_workflow(bestTune)%>%
fit(data=train_data)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id)) %>%
rename(type = .pred_class) %>%
select(id, type)
vroom_write(x = final_predictions, file = "./ggg_rf_model_preds_a.csv", delim = ",")
library(discrim)
library(glmnet)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(ggplot2)
library(recipes)
library(embed)
train_data <- vroom("train.csv")
test_data <- vroom("test.csv")
train_data <- train_data %>%
mutate(
type = as.factor(type),
color = as.factor(color))
test_data <- test_data %>%
mutate(color = as.factor(color))
## nb model
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create recipe
my_recipe <- recipe(ACTION ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Target encoding
step_lencode_glm(all_nominal_predictors(), outcome = vars(ACTION))
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
## Grid of values to tune over
tuning_grid <- grid_regular(
Laplace(range = c(0, 1)),
smoothness(range = c(0,1)),
levels = 4
)
## Split data for CV
folds <- vfold_cv(train_data, v = 4, repeats=1)
## Run the CV
CV_results <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(roc_auc, accuracy))
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <-
nb_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_data)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id)) %>%
rename(type = .pred_class) %>%
select(id, type)
# Export processed dataset
vroom_write(x = final_predictions, file = "./ggg_nb_model_preds_a.csv", delim = ",")
# Create recipe
my_recipe <- recipe(type ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Target encoding
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_smote(all_outcomes(), neighbors=2)
library(themis)
train_data <- train_data %>%
mutate(
type = as.factor(type),
color = as.factor(color))
test_data <- test_data %>%
mutate(color = as.factor(color))
## nb model
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create recipe
my_recipe <- recipe(type ~ ., data = train_data) %>%
# Collapse rare categories (<0.1%)
step_other(all_nominal_predictors(), threshold = 0.001, other = "other") %>%
# Target encoding
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_smote(all_outcomes(), neighbors=2)
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
## Grid of values to tune over
tuning_grid <- grid_regular(
Laplace(range = c(0, 1)),
smoothness(range = c(0,1)),
levels = 4
)
## Split data for CV
folds <- vfold_cv(train_data, v = 4, repeats=1)
## Run the CV
CV_results <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(roc_auc, accuracy))
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <-
nb_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_data)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id)) %>%
rename(type = .pred_class) %>%
select(id, type)
# Export processed dataset
vroom_write(x = final_predictions, file = "./ggg_nb_model_preds_b.csv", delim = ",")
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
## Grid of values to tune over
tuning_grid <- grid_regular(
Laplace(range = c(0, 1)),
smoothness(range = c(0,1)),
levels = 5
)
## Split data for CV
folds <- vfold_cv(train_data, v = 4, repeats=2)
## Run the CV
CV_results <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics = metric_set(roc_auc, accuracy))
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric = "roc_auc")
## Finalize the Workflow & fit it
final_wf <-
nb_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_data)
## Predict
final_predictions <- final_wf %>%
predict(new_data = test_data) %>%
bind_cols(test_data %>% select(id)) %>%
rename(type = .pred_class) %>%
select(id, type)
# Export processed dataset
vroom_write(x = final_predictions, file = "./ggg_nb_model_preds_c.csv", delim = ",")
